{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pip install nltk sklearn\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "def fixed_size_chunking(content, chunk_size=500):\n",
    "    words = content.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "\n",
    "\n",
    "def semantic_chunking(content):\n",
    "    # Ensure you've downloaded the 'punkt' tokenizer model\n",
    "    nltk.download('punkt')\n",
    "    # Split the content into sentences\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    paragraphs = []\n",
    "    current_paragraph = []\n",
    "\n",
    "    # Combine sentences into paragraphs (based on full stops or empty lines)\n",
    "    for sent in sentences:\n",
    "        current_paragraph.append(sent)\n",
    "        # Assuming a paragraph ends with a period or empty line (this can be tweaked)\n",
    "        if sent.endswith('.') or len(sent.strip()) == 0:\n",
    "            paragraphs.append(\" \".join(current_paragraph))\n",
    "            current_paragraph = []\n",
    "\n",
    "    # If there are remaining sentences that don't form a complete paragraph\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "\n",
    "def question_based_chunking(content, question):\n",
    "    # Use NLTK's semantic chunking to get paragraphs or sentences\n",
    "    chunks = semantic_chunking(content)\n",
    "\n",
    "    # Use TF-IDF to calculate the similarity between the question and each chunk\n",
    "    vectorizer = TfidfVectorizer().fit_transform([question] + chunks)\n",
    "    vectors = vectorizer.toarray()\n",
    "    \n",
    "    # Calculate cosine similarity between the question and each chunk\n",
    "    cosine_similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
    "    \n",
    "    # Select chunks with the highest similarity scores (you can adjust how many chunks to return)\n",
    "    relevant_chunks = [chunks[i] for i in cosine_similarities.argsort()[::-1][:5]]\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Remove unnecessary HTML tags, ads, etc.\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def call_firework_api(chunk, question, api_key):\n",
    "    url = \"https://api.firework.com/generate\"  # replace with actual API endpoint\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    data = {\n",
    "        \"chunk\": chunk,\n",
    "        \"question\": question\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_url_content(url, api_key):\n",
    "    serp_url = f\"https://serpapi.com/search?url={url}&api_key={api_key}\"\n",
    "    response = requests.get(serp_url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from firework_api import call_firework_api\n",
    "from serp_api import get_url_content\n",
    "from content_preprocessor import clean_content\n",
    "from chunking_algorithms import fixed_size_chunking, semantic_chunking, question_based_chunking\n",
    "\n",
    "\n",
    "def _select_best_answer(answers):\n",
    "    # Sort answers by relevance and return the most relevant one\n",
    "    best_answer = max(answers, key=lambda x: x.get('relevance', 0))\n",
    "    return best_answer['text'] if 'text' in best_answer else \"No relevant answer found\"\n",
    "\n",
    "def generate_answer(url, question, api_key_serp, api_key_firework):\n",
    "    # Retrieve and clean content\n",
    "    raw_content = get_url_content(url, api_key_serp)\n",
    "    clean_text = clean_content(raw_content)\n",
    "\n",
    "    # Apply chunking methods\n",
    "    fixed_chunks = fixed_size_chunking(clean_text)\n",
    "    semantic_chunks = semantic_chunking(clean_text)\n",
    "    question_chunks = question_based_chunking(clean_text, question)\n",
    "\n",
    "    # Combine chunks for processing (or prioritize one type)\n",
    "    all_chunks = fixed_chunks + semantic_chunks + question_chunks\n",
    "\n",
    "    # Send each chunk to Firework API and collect responses\n",
    "    answers = []\n",
    "    for chunk in all_chunks:\n",
    "        answer = call_firework_api(chunk, question, api_key_firework)\n",
    "        answers.append(answer)\n",
    "\n",
    "    # Evaluate and select the best answer\n",
    "    return _select_best_answer(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from rag_tool import generate_answer\n",
    "\n",
    "def test_chunking_methods(urls, questions, api_key_serp, api_key_firework):\n",
    "    for url, question in zip(urls, questions):\n",
    "        answer = generate_answer(url, question, api_key_serp, api_key_firework)\n",
    "        print(f\"URL: {url}, Question: {question}, Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
